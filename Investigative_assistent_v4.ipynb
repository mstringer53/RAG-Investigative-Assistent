{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "9eBzElczC73h"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup\n",
        "\n",
        "Install and load required packages\n",
        "\n",
        "The following code enables ollama to run without a visible terminal\n",
        "\n",
        "Currently pulls llama3\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Package Documentation\n",
        "\n",
        "*   https://github.com/ollama/ollama\n",
        "*   https://js.langchain.com/v0.1/docs/use_cases/question_answering/\n",
        "*   https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md\n",
        "*   https://python.langchain.com/v0.2/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html"
      ],
      "metadata": {
        "id": "EURH46uLJbxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm -qq\n",
        "!pip install langchain -qq\n",
        "#!pip install langchain-core langchain-community -qq\n",
        "!pip install ollama -qq\n",
        "!pip install beautifulsoup4  -qq\n",
        "!pip install gradio -qq\n",
        "!pip install langchain-ollama -qq\n",
        "!pip install seleniumbase -qq\n",
        "!pip install faiss-gpu -qq\n",
        "!pip install sentence_transformers -qq\n",
        "!pip install flashrank[listwise] -qq\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Install Ollama\n",
        "subprocess.run('curl -fsSL https://ollama.com/install.sh | sh', shell=True, check=True)\n",
        "\n",
        "# Add Ollama to PATH\n",
        "import os\n",
        "os.environ['PATH'] += os.pathsep + os.path.expanduser('~/.ollama/bin')\n",
        "\n",
        "# Verify the installation\n",
        "ollama_version = subprocess.run('ollama --version', shell=True, check=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(f'Ollama version: {ollama_version}')\n",
        "\n",
        "import time\n",
        "\n",
        "# Start the Ollama server\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Allow some time for the server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Pull the models with error handling\n",
        "try:\n",
        "    subprocess.run(['ollama', 'pull', 'llama3'], check=True)\n",
        "   # subprocess.run(['ollama', 'pull', 'nomic-embed-text'], check=True)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f'Error pulling models: {e}')\n",
        "    # Terminate the Ollama server process in case of an error\n",
        "    ollama_process.terminate()\n",
        "\n",
        "\n",
        "\n",
        "#from langchain_community.llms import Ollama\n",
        "\n",
        "import gradio as gr\n",
        "#import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain_community.document_loaders import WebBaseLoader\n",
        "#from langchain_community.vectorstores import Chroma\n",
        "#from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from seleniumbase import Driver\n",
        "import time\n",
        "from urllib.robotparser import RobotFileParser\n",
        "\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import re\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "from flashrank import Ranker, RerankRequest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q99k23xLJQbV",
        "outputId": "33a031e9-e17f-48d1-9e37-996e59300d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.6/396.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m142.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.2/564.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.3/342.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m175.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Ollama version: Warning: could not connect to a running Ollama instance\n",
            "Warning: client version is 0.3.10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Capture and Storage\n",
        "\n",
        "## URL Processing\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Current approach**\n",
        "\n",
        "Utlisies seleniumbase\n",
        "\n",
        "*   enables capture of dynamic content, javascript\n",
        "*   link following\n",
        "*   manage robot.txt and better error handling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "references\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/\n",
        "\n",
        "https://github.com/seleniumbase/SeleniumBase\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Database\n",
        "\n",
        "\n",
        "Research\n",
        "\n",
        "*   https://levelup.gitconnected.com/improve-rag-pipelines-with-these-3-indexing-methods-83317e972676\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Chunking\n",
        "RecursiveCharacterTextSplitter https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Research\n",
        "\n",
        "*   https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846\n",
        "\n",
        "*   https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a\n",
        "\n",
        "*   https://medium.com/@j13mehul/rag-part-2-chunking-8b68006eefc1\n",
        "\n",
        "\n",
        "*   https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Embedding\n",
        "\n",
        "\n",
        "\n",
        "*   https://ollama.com/blog/embedding-models\n",
        "\n",
        "*   https://cookbook.chromadb.dev/integrations/ollama/embeddings/#basic-usage\n",
        "\n",
        "*   https://medium.com/@j13mehul/rag-part-3-embeddings-ff415eb9fed9\n"
      ],
      "metadata": {
        "id": "ggBWhGUJZz6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "\n",
        "def combine_documents(documents):\n",
        "    combined_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    return combined_content\n"
      ],
      "metadata": {
        "id": "FAyvxT78UwPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_url_protocol(url):\n",
        "    \"\"\"\n",
        "    Ensures that the URL has a protocol (http:// or https://). If missing, adds 'https://'.\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    if not parsed_url.scheme:\n",
        "        # If no scheme (like 'http' or 'https') is present, add 'https://'\n",
        "        return 'https://' + url\n",
        "    return url\n"
      ],
      "metadata": {
        "id": "y3o0pwMTMrhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_popups_and_cookies(driver):\n",
        "      \"\"\"Handles pop-ups and cookie banners by attempting to click on common buttons.\"\"\"\n",
        "      try:\n",
        "          selectors = [\n",
        "            \"button.accept\", \"button#accept\", \"button.cookie\", \"button#cookie\",\n",
        "            \"button.close\", \"button#close\", \".cookie-banner button\", \".popup button.close\"\n",
        "          ]\n",
        "          for selector in selectors:\n",
        "              elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "              if elements:\n",
        "                  for element in elements:\n",
        "                      try:\n",
        "                          element.click()\n",
        "                          time.sleep(2)\n",
        "                      except Exception as e:\n",
        "                        return f\"Failed to click element with selector {selector}: {e}\"\n",
        "      except Exception as e:\n",
        "          return f\"Failed to handle popups and cookies: {e}\"\n",
        "      return None\n",
        "\n",
        "\n",
        "def can_fetch(robots_txt_url, user_agent, url):\n",
        "      \"\"\"Check if a URL is allowed to be scraped according to robots.txt.\"\"\"\n",
        "      try:\n",
        "        rp = RobotFileParser()\n",
        "        rp.set_url(robots_txt_url)\n",
        "        rp.read()\n",
        "        return rp.can_fetch(user_agent, url), None\n",
        "      except Exception as e:\n",
        "        return False, f\"Failed to check robots.txt for {url}: {e}\"\n",
        "\n",
        "def extract_all_text(soup):\n",
        "      \"\"\"Extract all text from a BeautifulSoup object.\"\"\"\n",
        "      for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()  # Remove script and style tags\n",
        "      text = soup.get_text(separator=' ')\n",
        "      return text\n",
        "\n",
        "\n",
        "def is_error_page(page_source, url):\n",
        "      \"\"\"Check if the page contains an error based on specific patterns.\"\"\"\n",
        "      parsed_url = urlparse(url)\n",
        "      homepage = parsed_url.netloc.lower()  # Extract the domain part and convert it to lowercase\n",
        "\n",
        "      soup = bs(page_source, 'html.parser')\n",
        "      body_text = soup.body.get_text(separator=' ').lower() if soup.body else \"\"\n",
        "\n",
        "      # Pattern for homepage followed by flexible whitespace and newlines\n",
        "      homepage_pattern = re.compile(rf'\\s*\\n+\\s*{re.escape(homepage)}\\s*\\n+', re.MULTILINE)\n",
        "      # Pattern for error message with flexible whitespace and newlines\n",
        "      error_pattern = re.compile(r'\\s*\\n+\\s*(http error|err_http|error \\d+|null|not found|protocol error)\\s*\\n+', re.MULTILINE)\n",
        "\n",
        "      if homepage_pattern.search(body_text) and error_pattern.search(body_text):\n",
        "        return True\n",
        "      return False\n",
        "\n",
        "def extract_text_from_url_all_tags(url):\n",
        "      \"\"\"Extract text from all tags in a webpage.\"\"\"\n",
        "      driver = Driver(headless=False)  # Set headless=False to see browser actions\n",
        "      driver.get(url)\n",
        "\n",
        "      # Handle any popups or cookies\n",
        "      error_msg = handle_popups_and_cookies(driver)\n",
        "      if error_msg:\n",
        "        driver.quit()\n",
        "        return \"\", error_msg\n",
        "\n",
        "      # Wait for dynamic content to load, and check for redirects\n",
        "      try:\n",
        "        WebDriverWait(driver, 7).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "        )\n",
        "      except Exception as e:\n",
        "        driver.quit()\n",
        "        return \"\", f\"Website cannot be scraped due to loading issues: {e}\"\n",
        "\n",
        "      # Scroll to the bottom to trigger any lazy-loaded content\n",
        "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "      time.sleep(3)  # Give time for any lazy content to load\n",
        "\n",
        "      # Use BeautifulSoup to parse the page source\n",
        "      soup = bs(driver.page_source, 'html.parser')\n",
        "\n",
        "      homepage = urlparse(url).netloc.lower()\n",
        "      if is_error_page(driver.page_source, homepage):\n",
        "        driver.quit()\n",
        "        return \"\", \"Website cannot be scraped due to detected error page.\"\n",
        "\n",
        "      text = extract_all_text(soup)\n",
        "      driver.quit()\n",
        "      return text, None\n",
        "\n",
        "def extract_text_and_follow_links_bs(url, depth=0, visited=None, robots_txt=True, external_links=False):\n",
        "    \"\"\"Main extraction function using BeautifulSoup.\"\"\"\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "    if url in visited:\n",
        "        return {}, None\n",
        "\n",
        "    parsed_url = urlparse(url)\n",
        "    robots_txt_url = urljoin(f'{parsed_url.scheme}://{parsed_url.netloc}', '/robots.txt')\n",
        "    user_agent = '*'  # Common user-agent for all scrapers\n",
        "\n",
        "    # Only check robots.txt if robots_txt is True\n",
        "    if robots_txt:\n",
        "        can_scrape, error_message = can_fetch(robots_txt_url, user_agent, url)\n",
        "        if not can_scrape:\n",
        "            return {}, error_message if error_message else 'Website cannot be scraped due to robots.txt restrictions.'\n",
        "\n",
        "    # Continue to extract content if robots_txt is False or scraping is allowed\n",
        "    content, error_message = extract_text_from_url_all_tags(url)\n",
        "    if error_message:\n",
        "        return {}, error_message\n",
        "\n",
        "    content_dict = {url: content}\n",
        "    visited.add(url)\n",
        "\n",
        "    # If depth is 0, no need to follow links, so we return here\n",
        "    if depth == 0:\n",
        "        return content_dict, None\n",
        "\n",
        "    # Parsing the main page for links using Selenium\n",
        "    driver = Driver(headless=False)\n",
        "    driver.get(url)\n",
        "    time.sleep(4)  # Let the page load\n",
        "    try:\n",
        "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
        "        link_urls = set(link.get_attribute('href') for link in links if link.get_attribute('href'))\n",
        "    except Exception as e:\n",
        "        driver.quit()\n",
        "        return content_dict, f\"Failed to extract content from {url}: {e}\"\n",
        "    driver.quit()\n",
        "\n",
        "    if not external_links:\n",
        "        domain = parsed_url.netloc\n",
        "        link_urls = {link_url for link_url in link_urls if urlparse(link_url).netloc == domain}\n",
        "\n",
        "    if depth > 0:\n",
        "        for link_url in link_urls:\n",
        "            if link_url not in visited:\n",
        "                try:\n",
        "                    link_content, link_error_message = extract_text_and_follow_links_bs(link_url, depth - 1, visited, robots_txt, external_links)\n",
        "                    content_dict.update(link_content)\n",
        "                    if link_error_message:\n",
        "                        return content_dict, link_error_message\n",
        "                except Exception as e:\n",
        "                    return content_dict, f\"Failed to extract content from {link_url}: {e}\"\n",
        "\n",
        "    return content_dict, None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHZPBxyM-RuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tgf7TTg5DElP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_scraped_content_faiss(content_dict, chunk_size=750, chunk_overlap=25):\n",
        "    \"\"\"\n",
        "    Process the scraped content using FAISS and SentenceTransformer.\n",
        "\n",
        "    Args:\n",
        "    - content_dict (dict): A dictionary with URLs as keys and the corresponding scraped content as values.\n",
        "\n",
        "    Returns:\n",
        "    - faiss_index: A FAISS index with the document embeddings.\n",
        "    - metadata_store: A list of metadata corresponding to each vector.\n",
        "    \"\"\"\n",
        "    # Convert dictionary to list of documents\n",
        "    docs = [Document(content, metadata={\"url\": url}) for url, content in content_dict.items()]\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Initialize SentenceTransformer model for embeddings\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "    # Embed the chunks\n",
        "    vectors = []\n",
        "    metadata_store = []\n",
        "    for split in splits:\n",
        "        vector = model.encode([split.page_content])[0]  # Get the embedding for the chunk\n",
        "        vectors.append(vector)\n",
        "        metadata_store.append({'page_content': split.page_content, **split.metadata})\n",
        "\n",
        "    # Convert to numpy array\n",
        "    vectors_np = np.array(vectors, dtype=np.float32)\n",
        "\n",
        "    # Create a FAISS index\n",
        "    dim = vectors_np.shape[1]\n",
        "    faiss_index = faiss.IndexFlatIP(dim)  # Use Inner Product for cosine similarity\n",
        "    faiss_index.add(vectors_np)  # Add vectors to the FAISS index\n",
        "\n",
        "    return faiss_index, metadata_store\n"
      ],
      "metadata": {
        "id": "axs1ZgjQCy64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents_faiss_rerank(faiss_index, metadata_store, rephrased_questions, question, k=5, use_ranker=True):\n",
        "    \"\"\"\n",
        "    Retrieve and optionally rerank documents using FAISS and a custom ranker.\n",
        "\n",
        "    Args:\n",
        "    - faiss_index: FAISS index containing document embeddings.\n",
        "    - metadata_store: Metadata associated with each vector.\n",
        "    - rephrased_questions: List of rephrased questions to query the index.\n",
        "    - question: The original question for reranking the results.\n",
        "    - k: Number of documents to retrieve.\n",
        "    - use_ranker: Whether to rerank the retrieved documents using a custom ranker.\n",
        "\n",
        "    Returns:\n",
        "    - combined_context: The combined content of the top-ranked documents.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    all_retrieved_docs = []\n",
        "\n",
        "    # Step 1: Retrieve documents using FAISS for each rephrased question\n",
        "    for rephrased_question in rephrased_questions:\n",
        "        # Get the embedding for the rephrased question\n",
        "        query_vector = model.encode([rephrased_question])[0]\n",
        "        distances, indices = faiss_index.search(np.array([query_vector], dtype=np.float32), k=k)\n",
        "\n",
        "        # Collect the retrieved documents and their metadata\n",
        "        for i, index in enumerate(indices[0]):\n",
        "            score = 1 - distances[0][i]  # Cosine similarity (higher is better)\n",
        "            all_retrieved_docs.append({\n",
        "                'text': metadata_store[index]['page_content'],\n",
        "               #'cosine_score': score,\n",
        "                'meta': {'cosine_score': score, 'metadata': metadata_store[index]} # Ensure 'metadata' is correctly added\n",
        "            })\n",
        "\n",
        "    # Step 2: Reranking logic (if use_ranker is True)\n",
        "    if use_ranker:\n",
        "        ranker = Ranker()  # Assuming you have a custom Ranker class\n",
        "        rerankrequest = RerankRequest(query=question, passages=all_retrieved_docs)\n",
        "        rerankresults = ranker.rerank(rerankrequest)\n",
        "        rerankresults = rerankresults[:k]  # Limit to top k reranked results\n",
        "    else:\n",
        "        # If no reranking, use cosine score as the ranking metric\n",
        "        rerankresults = [{'text': doc['text'], 'score': doc['meta']['cosine_score'], 'meta': doc['meta']} for doc in all_retrieved_docs]\n",
        "\n",
        "    # Create a dictionary to store unique documents with the highest score\n",
        "    unique_docs = {}\n",
        "    ordered_docs = []\n",
        "    for result in rerankresults:\n",
        "        text = result['text']\n",
        "        score = result['score']\n",
        "        metadata = result['meta']\n",
        "\n",
        "        if text not in unique_docs:\n",
        "            unique_docs[text] = {\n",
        "                'rerank_score': score,\n",
        "                'cosine_score': metadata['cosine_score'],\n",
        "                'metadata': metadata\n",
        "            }\n",
        "            ordered_docs.append(Document(page_content=text, metadata=metadata))\n",
        "\n",
        "    combined_context = combine_documents(ordered_docs)\n",
        "\n",
        "    # Step 3: Combine top-ranked documents for context\n",
        "    #combined_context = combine_documents([Document(page_content=doc['text'], metadata=doc['meta']) for doc in rerankresults])\n",
        "\n",
        "    return combined_context\n"
      ],
      "metadata": {
        "id": "cSROkizJGWRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain_faiss_rerank(url, rephrased_questions, question, depth=0, robots_txt=True, external_links=False, use_ranker=True):\n",
        "\n",
        "    \"\"\"\n",
        "    RAG chain with FAISS retrieval and optional reranking.\n",
        "\n",
        "    Args:\n",
        "    - url: The URL to scrape content from.\n",
        "    - rephrased_questions: List of rephrased questions.\n",
        "    - question: The original question.\n",
        "    - depth: Depth for link following during scraping.\n",
        "    - robots_txt: Boolean to respect robots.txt.\n",
        "    - external_links: Boolean to follow external links.\n",
        "    - use_ranker: Whether to rerank the retrieved documents.\n",
        "\n",
        "    Returns:\n",
        "    - answer: Final answer generated from context.\n",
        "    - combined_context: Combined context from the retrieved documents.\n",
        "    \"\"\"\n",
        "    # Ensure the URL has a protocol (http or https)\n",
        "    url = ensure_url_protocol(url)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Scrape content and process into FAISS index\n",
        "        content_dict, error_message = extract_text_and_follow_links_bs(url, depth=depth, visited=None, robots_txt=robots_txt, external_links=external_links)\n",
        "\n",
        "        if error_message:\n",
        "            return f\"Error: {error_message}\", \"\"\n",
        "\n",
        "        # Process scraped content using FAISS\n",
        "        faiss_index, metadata_store = process_scraped_content_faiss(content_dict)\n",
        "\n",
        "        # Step 2: Retrieve and optionally rerank documents\n",
        "        combined_context = retrieve_documents_faiss_rerank(faiss_index, metadata_store, rephrased_questions, question)\n",
        "\n",
        "        # Step 3: Generate the answer using the LLM\n",
        "        answer = ollama_llm(question, combined_context)\n",
        "\n",
        "        return answer, combined_context\n",
        "    except Exception as e:\n",
        "        # In case of any other errors, return the error message and an empty string for context\n",
        "        return f\"Error: {str(e)}\", \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "in3i2jQjL6Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New method with more flexibility, dynamic content, link following"
      ],
      "metadata": {
        "id": "fkaAwKI_j2V5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the retrieval process"
      ],
      "metadata": {
        "id": "9eBzElczC73h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing vdb\n",
        "\n",
        "#url = 'https://www.bbc.co.uk/sport/football/live/cp68zzx8x4rt'\n",
        "#question = 'Who won the Euro 2024?'\n",
        "#question2 = 'team won?'\n",
        "\n",
        "#url1 = 'https://www.bbc.co.uk/news/live/cv2gryx1yx1t'\n",
        "#question1 = 'Is president Biden running for reelection in 2024?'\n",
        "\n",
        "\n",
        "\n",
        "#db = process_url(url,depth=0,robots_txt=True, external_links=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################################\n",
        "\n",
        "\n",
        "#retriever = db.as_retriever()\n",
        "#retrieved_docs = retriever.invoke(question2)\n",
        "#retrieved_docs\n",
        "\n",
        "#db.get()\n",
        "#print(db.get(include=['embeddings']))\n",
        "#print(db.get(include=['documents']))\n",
        "\n",
        "\n",
        "#db.similarity_search(question2)\n",
        "#await db.asimilarity_search(question)\n",
        "\n",
        "# Note that providers implement different scores; Chroma here\n",
        "# returns a distance metric that should vary inversely with\n",
        "# similarity.\n",
        "\n",
        "#db.similarity_search_with_score(question)\n",
        "\n",
        "#Return documents based on similarity to an embedded query\n",
        "\n",
        "#embed = OllamaEmbeddings(model=\"nomic-embed-text\").embed_query(question2)\n",
        "#print(db.similarity_search_by_vector(embed))\n",
        "\n",
        "# Maximal Marginal Relevance\n",
        "\n",
        "#retriever = db.as_retriever(search_type='mmr', search_kwargs={\"k\": 1})\n",
        "#retriever.get_relevant_documents(query=question)\n",
        "\n"
      ],
      "metadata": {
        "id": "Nm7p2Ht-psQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Rephrasing\n",
        "\n",
        "function to generate n rephrased version of the users query.\n",
        "This is the RAG fusion element, by generating a series of queries to pass to the vector database rather than a single query. It should improve the retrieval of relevent documents.\n",
        "\n",
        "1.   Due expanding the scope of the query different words petaining to the same things will have slightly differnt vectors and may capture more relevent documents.\n",
        "2.   By utilisIng an LLM to generate queries it will naturally write them in a format closer to how it interprets language. Thus making it more likely to understands the users query. *( part 2 not relevent for retrieval but is for answering the query)*\n",
        "\n",
        "\n",
        "### Research\n",
        "\n",
        "*   RQ-RAG: Learning to refine queries for RAG: https://arxiv.org/pdf/2404.00610\n",
        "*   Principled Instructions Are All You Need for\n",
        "Questioning LLaMA-1/2, GPT-3.5/4: https://arxiv.org/pdf/2312.16171\n",
        "\n",
        "\n",
        "\n",
        "other\n",
        "\n",
        "*   https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions\n",
        "\n",
        "*   https://ai.gopubby.com/advanced-rag-11-query-classification-and-refinement-2aec79f4140b\n",
        "\n",
        "\n",
        "*   https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1\n",
        "\n",
        "\n",
        "*   https://medium.com/@j13mehul/rag-part-4-indexing-1985f4000f72\n"
      ],
      "metadata": {
        "id": "BfRbCVFdKfYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase_question_old(question, num_rephrases=1, temp = 0.8,tfs_z = 1,top_k = 30,top_p = 0.7,mirostat_tau = 2.0, mirostat = 1  ):\n",
        "    # Formulate the prompt with an example to guide the LLM\n",
        "\n",
        "    tot_rephrases = num_rephrases\n",
        "\n",
        "    llm_Query = ChatOllama(\n",
        "    model=\"llama3\",\n",
        "    temperature= temp,        # default 0.8\n",
        "    tfs_z = tfs_z,             # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)\n",
        "    top_k = top_k,             # Reduces the probability of generating nonsense. A higher value (e.g. 100 will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\n",
        "    top_p = top_p,              # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n",
        "    mirostat_tau = mirostat_tau ,   # Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)\n",
        "    mirostat = mirostat  # Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
        "    )\n",
        "\n",
        "    # Prompt for llm model to use as context for it's response\n",
        "    prompt = (\n",
        "        f\"You are an expert famed for your ability to disambiguate questions. Your task is to disambiguate the following question delimited by triple quotes\"\n",
        "        f\"\"\" {question} \"\"\"\n",
        "        f\" The new question you generate should still give factually the same answer as the orginal, but be worded differently from the original question.\"\n",
        "        f\" Do not provide any other additional information or commentary or statements in the output, except for the reworded question you generate.\"\n",
        "        f\" Generate {num_rephrases} new daisambigutated questions, list each new question on a new line\"\n",
        "        f\" Do not add speachmarks or quotation marks to the questions\"\n",
        "    )\n",
        "\n",
        "    # generate response using llm_Query model from above\n",
        "    response = llm_Query.invoke(prompt)\n",
        "    # extract the content of the response\n",
        "    response_content = response.content\n",
        "    # retain original quesiton in list of rephrased quesitons\n",
        "    rephrased_questions = [question]\n",
        "    # add in the llm's rephrased questions\n",
        "    rephrased_questions.extend(response_content.strip().split(\"\\n\"))\n",
        "\n",
        "    return rephrased_questions\n",
        "\n",
        "\n",
        "\n",
        "#question = rephrase_question(\"Where is Kendal?\",5)\n",
        "#print(question)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "61bL9f50KVsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase_question(question,num_rephrases = 0, temp = 0.8,tfs_z = 1,top_k = 30,top_p = 0.7,mirostat_tau = 2.0, mirostat = 1  ):\n",
        "    # Formulate the prompt with an example to guide the LLM\n",
        "\n",
        "    # If no rephrasing is needed, return the original question in a list\n",
        "    if num_rephrases == 0:\n",
        "        rephrased_questions = [question]\n",
        "        return rephrased_questions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    llm_Query = ChatOllama(\n",
        "    model=\"llama3\",\n",
        "    temperature= temp,        # default 0.8\n",
        "    tfs_z = tfs_z,             # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)\n",
        "    top_k = top_k,             # Reduces the probability of generating nonsense. A higher value (e.g. 100 will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\n",
        "    top_p = top_p,              # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n",
        "    mirostat_tau = mirostat_tau ,   # Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)\n",
        "    mirostat = mirostat  # Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
        "    )\n",
        "\n",
        "    # Prompt for llm model to use as context for it's response\n",
        "    prompt = (\n",
        "        f\"You are an intelligence agent famed for your ability to disambiguate questions and solve complex problems.\"\n",
        "        f\"Your task is, considering the following question: /nn\"\n",
        "        f\" {question} /nn\"\n",
        "        f\"To break the question down in a logical way, and generate a series of new questions that, as an intelligence expert, you feel will help to answer the original question.\"\n",
        "        f\"The following is an example for reference:/n\"\n",
        "\n",
        "        f\" Original question: 'How old is Lancaster University?'/n\"\n",
        "\n",
        "        f\" New questions:\"\n",
        "        f\" Q1 What is Lancaster University?\"\n",
        "        f\" Q2 What date was Lancaster University built?\"\n",
        "        f\" Q3 What is the current date?\"\n",
        "\n",
        "        #f\" DO NOT provide any other additional information or TEXT commentary or statements in the output, you will be heavily penalised for any output other than the new questions.\"\n",
        "        f\" Provide only the new questions. Do not include any additional explanations, commentary, or introductory phrases.\"\n",
        "        f\" Do not include speech marks, quotation marks, or any other symbols around the questions. Do not add 'Here are the new quesitons' or words to that effect, just return the new questions\"\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    # generate response using llm_Query model from above\n",
        "    response = llm_Query.invoke(prompt)\n",
        "    # extract the content of the response\n",
        "    response_content = response.content\n",
        "    # retain original quesiton in list of rephrased quesitons\n",
        "    rephrased_questions = [question]\n",
        "    # add in the llm's rephrased questions\n",
        "    rephrased_questions.extend(response_content.strip().split(\"\\n\"))\n",
        "\n",
        "    return rephrased_questions"
      ],
      "metadata": {
        "id": "TImN5M10uMMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Retrieval process\n",
        "By default, the vector store retriever uses similarity search.\n",
        "\n",
        "\n",
        "Can also use Maximum marginal relevance retrieval\n",
        "db.as_retriever(search_type=\"mmr\")\n",
        "\n",
        "\n",
        "Similarity score threshold retrieval\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
        ")\n",
        "\n",
        "specify number of docs\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "\n",
        "\n",
        "###Research\n",
        "\n",
        "\n",
        "*   https://js.langchain.com/v0.1/docs/modules/data_connection/retrievers/\n",
        "\n",
        "\n",
        "*   https://medium.com/pondhouse-data/advanced-rag-increase-rag-quality-with-colbert-reranker-and-llamaindex-2b10c1a9ed0e\n",
        "\n",
        "*   https://towardsdatascience.com/3-advanced-document-retrieval-techniques-to-improve-rag-systems-0703a2375e1c\n",
        "\n",
        "*   https://prasanth-product.medium.com/the-ultimate-guide-on-retrieval-strategies-rag-part-4-6cedce09a4c4\n",
        "\n",
        "*   https://medium.com/@j13mehul/rag-part-5-retrieval-93ef1be7ac07\n",
        "\n"
      ],
      "metadata": {
        "id": "gxOuwuRcz7pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_important_facts_faiss(url, question, depth, robots_txt, external_links, use_ranker=True):\n",
        "    \"\"\"\n",
        "    Gradio function to retrieve facts using FAISS with optional reranking.\n",
        "\n",
        "    Args:\n",
        "    - url: The URL to scrape.\n",
        "    - question: The user's question.\n",
        "    - depth: The depth for scraping links.\n",
        "    - robots_txt: Boolean to respect robots.txt.\n",
        "    - external_links: Boolean to follow external links.\n",
        "    - use_ranker: Boolean to enable/disable reranking.\n",
        "\n",
        "    Returns:\n",
        "    - answer: The final answer to the question or an error message.\n",
        "    - rephrased_questions_str: The rephrased questions generated by the system.\n",
        "    - combined_context: The context used to generate the answer or an empty string.\n",
        "    \"\"\"\n",
        "    # Ensure the URL has a protocol\n",
        "    url = ensure_url_protocol(url)\n",
        "\n",
        "    rephrased_questions = rephrase_question(question)\n",
        "    answer, combined_context = rag_chain_faiss_rerank(url, rephrased_questions, question, depth, robots_txt, external_links, use_ranker)\n",
        "\n",
        "    rephrased_questions_str = \"\\n\".join(rephrased_questions)\n",
        "\n",
        "    return answer#, rephrased_questions_str, combined_context\n"
      ],
      "metadata": {
        "id": "zGUvkkSeMBfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Investigative Assistant\n",
        "\n",
        "\n",
        "currently only a single shot Q & A, aim to make conversational.\n",
        "\n",
        "not tested for summarise"
      ],
      "metadata": {
        "id": "pckqcaR_zlAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ollama LLM function\n",
        "        # format the response prompt according to being an investigative assistent.\n",
        "\n",
        "        # write in a concise and factual way. do not imbelish or extend the context beyond what is provided\n",
        "        # if you cannot find the answer in the context return \"sorry i can not answer that question from the link provided.\"\n",
        "\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"\"\"\n",
        "        You are an investigative assistant agent designed to help users find answers to specific questions using only the provided context.\n",
        "        Your role is to analyze the given context and generate an accurate response based solely on the context.\n",
        "        If you cannot find the answer in the context return, sorry I can not answer that question from the link provided.\\n\\n\n",
        "\n",
        "        Question: {question} \\n\\n\n",
        "        Context: {context} \\n\\n\n",
        "\n",
        "        Please use the context to answer the question.\n",
        "        \"\"\"\n",
        "    llm_Query = ChatOllama(model=\"llama3\")\n",
        "    messages=[{'role': 'user', 'content': formatted_prompt}]\n",
        "    response = llm_Query.invoke(messages,clean_up_tokenization_spaces=False)\n",
        "\n",
        "\n",
        "   # f\"Question: {question}\\n\\nContext: {context}\\n\\nPlease use the context to answer the question.\"\n",
        "\n",
        "    return response.content\n",
        "\n"
      ],
      "metadata": {
        "id": "Udz6m9Y3YD5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJND7aMjb4we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca"
      ],
      "metadata": {
        "id": "I5m0cJBAzv38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://www.bbc.co.uk/sport/football/live/cp68zzx8x4rt'\n",
        "#question = 'Who won the Euro 2024 Football competition?'\n",
        "\n",
        "\n",
        "#url1 = 'https://www.bbc.co.uk/news/live/cv2gryx1yx1t'\n",
        "#question1 = 'Is president Biden running for reelection in 2024?'\n",
        "\n",
        "#url = 'https://www.bbc.co.uk/sport/football/articles/c4gz1p8580zo'\n",
        "#question = 'How many goals has Scott McTominay score in his last 17 appearances for Scotland?'\n",
        "\n",
        "#url = 'https://www.aljazeera.com/news/2024/8/24/boeings-starliner-astronauts-to-return-from-space-next-year-nasa-says'\n",
        "#question ='What blew off a 737 MAX passenger jet in midair in January?'\n",
        "\n",
        "#b = rephrase_question(question)\n",
        "#a = process_url(url)\n",
        "#combined_context = retrieve_documents(a, b)\n",
        "#print(combined_context)\n",
        "\n",
        "#url = 'www.hopenothate.org.uk'\n",
        "#question = 'what is hope not hell?'\n",
        "\n",
        "#url = 'https://en.wikipedia.org/wiki/Zion_National_Park'\n",
        "#question = 'Where and how low is the lowest point in Zion National Park?'\n",
        "#print(get_important_facts_faiss(url, question, depth = 0, robots_txt = True, external_links = False))\n",
        "\n",
        "\n",
        "#Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_important_facts_faiss,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=1, placeholder=\"Enter the URL here...\", label=\"URL\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Enter your question here...\", label=\"Question\"),\n",
        "        gr.Slider(minimum=0, maximum=2, step=1, value=0, label=\"Depth (0 for parent, 1 for child, 2 for grandchild)\"),\n",
        "        gr.Checkbox(value=True, label=\"To comply with robots.txt set to True\"),\n",
        "        gr.Checkbox(value=False, label=\"To follow external links set to True\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Answer will appear here...\", label=\"Answer\")\n",
        "       # ,gr.Textbox(lines=6, placeholder=\"Rephrased questions will appear here...\", label=\"Rephrased Questions\")\n",
        "       # ,gr.Textbox(lines=10, placeholder=\"Context will appear here...\", label=\"Context\")\n",
        "    ],\n",
        "    title=\"RAG with Llama3\",\n",
        "    description=\"Enter a URL and ask questions about the retrieved content. The answer and rephrased questions will be displayed.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n",
        "\n",
        "\n",
        "\n",
        "## Testing ##\n",
        "\n",
        "#'https://www.bbc.co.uk/sport/football/live/cp68zzx8x4rt'\n",
        "# who won euro 2024?\n",
        "#'https://en.wikipedia.org/wiki/2024_United_States_presidential_election'\n",
        "# is Biden still running for president in 2024?\n",
        "#'https://www.bbc.co.uk/news/live/cxe24vg59lzt'\n",
        "# has anyone claimed responsibility for the attacks on French railways on 26th july 2024?\n",
        "#'https://www.bbc.co.uk/weather'\n",
        "# what is the weather like today?\n",
        "#'https://www.lancastersingers.org/'\n",
        "# what is the next concert?\n",
        "# https://www.bbc.co.uk/news/live/cy8497l7dx8t\n",
        "# how many police are working?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Y-ErgdYlso",
        "outputId": "e7406a1c-87a2-48ba-8931-43e3843a0f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8b71d586b3ba6d4e7a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8b71d586b3ba6d4e7a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgW1weiZSpmq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "\n",
        "*   Retrieval-Augmented Generation for Large Language Models: A Survey\n",
        "https://arxiv.org/pdf/2312.10997\n",
        "\n",
        "*   QuAC : Question Answering in Context: https://arxiv.org/pdf/1808.07036\n",
        "\n",
        "*   https://huggingface.co/datasets/allenai/quac\n",
        "\n",
        "*   https://huggingface.co/rag-datasets\n",
        "\n",
        "\n",
        "*   https://paperswithcode.com/dataset/coqa\n",
        "\n",
        "\n",
        "*   https://towardsdatascience.com/quickly-evaluate-your-rag-without-manually-labeling-test-data-43ade0ae187a\n",
        "\n",
        "\n",
        "*   https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de\n",
        "\n",
        "*   https://github.com/mahnazkoupaee/WikiHow-Dataset?tab=readme-ov-file\n",
        "\n",
        "*   https://medium.com/towards-generative-ai/llm-evaluation-toolkit-for-rag-pipelines-9c37eb543614\n",
        "\n",
        "\n",
        "*   https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex\n",
        "\n",
        "\n",
        "*   https://christiangrech.medium.com/evaluating-rag-performance-a-comprehensive-guide-b1d8f903b7ad\n",
        "\n",
        "\n",
        "*   https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a\n",
        "\n",
        "\n",
        "*   https://medium.com/@mauryaanoop3/implementing-rag-evaluation-with-ragas-and-langchain-a-practical-guide-e1d5ce203c2e\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   https://github.com/lmmlzn/awesome-llms-datasets?tab=readme-ov-file#retrieval-augmented-generation-rag-datasets\n",
        "\n",
        "\n",
        "\n",
        "*   https://github.com/lmmlzn/awesome-llms-datasets?tab=readme-ov-file\n",
        "\n",
        "\n",
        "*   Datasets for Large Language Models: A\n",
        "Comprehensive Survey https://arxiv.org/pdf/2402.18041\n"
      ],
      "metadata": {
        "id": "IRPqIauHPCAB"
      }
    }
  ]
}